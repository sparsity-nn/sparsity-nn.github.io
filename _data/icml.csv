title,author,year,abstract,paper_url,code_url
Proving the lottery ticket hypothesis: Pruning is all you need,"Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, Ohad Shamir",2020,"The lottery ticket hypothesis (Frankle and Carbin, 2018), states that a randomly-initialized network contains a small subnetwork such that, when trained in isolation, can compete with the performance of the original network. We prove an even stronger hypothesis (as was also conjectured in Ramanujan et al., 2019), showing that for every bounded distribution and every target network with bounded weights, a sufficiently over-parameterized neural network with random weights contains a subnetwork with roughly the same accuracy as the target network, without any further training.",http://proceedings.mlr.press/v119/malach20a.html,
Linear Mode Connectivity and the Lottery Ticket Hypothesis,"Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, Michael Carbin",2020,"We study whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise (e.g., random data order and augmentation). We find that standard vision models become stable to SGD noise in this way early in training. From then on, the outcome of optimization is determined to a linearly connected region. We use this technique to study iterative magnitude pruning (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained in isolation to full accuracy. We find that these subnetworks only reach full accuracy when they are stable to SGD noise, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (ResNet-50 and Inception-v3 on ImageNet).",http://proceedings.mlr.press/v119/frankle20a.html,
