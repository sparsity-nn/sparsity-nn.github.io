title,author,venue,year,paper_url
A Signal Propagation Perspective for Pruning Neural Networks at Initialization,"Namhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould, Philip H. S. Torr",ICLR,2020,https://arxiv.org/abs/1906.06307
Comparing Rewinding and Fine-tuning in Neural Network Pruning,"Alex Renda, Jonathan Frankle, Michael Carbin",ICLR,2020,https://arxiv.org/abs/2003.02389
Data-Independent Neural Pruning via Coresets,"Ben Mussay, Margarita Osadchy, Vladimir Braverman, Samson Zhou, Dan Feldman",ICLR,2020,https://arxiv.org/abs/1907.04018
Dynamic Channel Pruning: Feature Boosting and Suppression,"Xitong Gao, Yiren Zhao, _ukasz Dudziak, Robert Mullins, Cheng-zhong Xu",ICLR,2019,https://arxiv.org/abs/1810.05331
Dynamic Model Pruning with Feedback,"Tao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, Martin Jaggi",ICLR,2020,https://openreview.net/forum?id=SJem8lSFwB
Dynamic Sparse Graph for Efficient Deep Learning,"Liu Liu, Lei Deng, Xing Hu, Maohua Zhu, Guoqi Li, Yufei Ding, Yuan Xie",ICLR,2019,https://arxiv.org/abs/1810.00859
Lookahead: A Far-sighted Alternative of Magnitude-based Pruning,"Sejun Park, Jaeho Lee, Sangwoo Mo, Jinwoo Shin",ICLR,2020,https://arxiv.org/abs/2002.04809
One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation,"Matthew Shunshi Zhang, Bradly Stadie",ICLR,2020,https://arxiv.org/abs/1912.00120
Provable Filter Pruning for Efficient Neural Networks,"Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, Daniela Rus",ICLR,2020,https://arxiv.org/abs/1911.07412
ProxSGD: Training Structured Neural Networks under Regularization and Constraints,"Yang Yang, Yaxiong Yuan, Avraam Chatzimichailidis, Ruud JG van Sloun, Lei Lei, Symeon Chatzinotas",ICLR,2020,https://openreview.net/forum?id=HygpthEtvr
Rethinking the Value of Network Pruning,"Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, Trevor Darrell",ICLR,2019,https://arxiv.org/abs/1810.05270
SNIP: Single-shot Network Pruning based on Connection Sensitivity,"Namhoon Lee, Thalaiyasingam Ajanthan, Philip H. S. Torr",ICLR,2019,https://arxiv.org/abs/1810.02340
"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks","Jonathan Frankle, Michael Carbin",ICLR,2019,https://arxiv.org/abs/1803.03635
Adversarial Neural Pruning with Latent Vulnerability Suppression,"Divyam Madaan, EJinwoo Shin, ESung Ju Hwang",ICML,2020,https://arxiv.org/abs/1908.04355
Approximated Oracle Filter Pruning for Destructive CNN Width Optimization github,"Xiaohan Ding,EGuiguang Ding,EYuchen Guo,EJungong Han,EChenggang Yan",ICML,2019,https://arxiv.org/abs/1905.04748
Collaborative Channel Pruning for Deep Networks,"Hanyu Peng,EJiaxiang Wu,EShifeng Chen,EJunzhou Huang",ICML,2019,http://proceedings.mlr.press/v97/peng19c.html
DropNet: Reducing Neural Network Complexity via Iterative Pruning,"Chong Min John Tan,EMehul Motani",ICML,2020,https://proceedings.icml.cc/static/paper_files/icml/2020/2026-Paper.pdf
EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis,"Chaoqi Wang,ERoger Grosse,ESanja Fidler,EGuodong Zhang",ICML,2019,https://arxiv.org/abs/1905.05934
Linear Mode Connectivity and the Lottery Ticket Hypothesis,"Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, Michael Carbin",ICML,2020,http://proceedings.mlr.press/v119/frankle20a.html
Network Pruning by Greedy Subnetwork Selection,"Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, Qiang Liu",ICML,2020,https://arxiv.org/abs/2003.01794
Operation-Aware Soft Channel Pruning using Differentiable Masks,"Minsoo Kang,EBohyung Han",ICML,2020,https://arxiv.org/abs/2007.03938
Proving the lottery ticket hypothesis: Pruning is all you need,"Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, Ohad Shamir",ICML,2020,http://proceedings.mlr.press/v119/malach20a.html
Proving the Lottery Ticket Hypothesis: Pruning is All You Need,"Eran Malach,EGilad Yehudai,EShai Shalev-Schwartz,EOhad Shamir",ICML,2020,https://arxiv.org/abs/2002.00585
Soft Threshold Weight Reparameterization for Learnable Sparsity,"Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, Ali Farhadi",ICML,2020,https://arxiv.org/abs/2002.03231
AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters,"Xia Xiao, Zigeng Wang, Sanguthevar Rajasekaran",NIPS,2019,https://papers.nips.cc/paper/9521-autoprune-automatic-network-pruning-by-regularizing-auxiliary-parameters
Bayesian Bits: Unifying Quantization and Pruning,"Mart van Baalen, Christos Louizos, Markus Nagel, Rana Ali Amjad, Ying Wang, Tijmen Blankevoort, Max Welling",NIPS,2020,https://arxiv.org/abs/2005.07093
"Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask","Hattie Zhou, Janice Lan, Rosanne Liu, Jason Yosinski",NIPS,2019,https://arxiv.org/abs/1905.01067
Directional Pruning of Deep Neural Networks,"Shih-Kang Chao, Zhanyu Wang, Yue Xing, Guang Cheng",NIPS,2020,https://arxiv.org/abs/2006.09358
Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks,"Zhonghui You, Kun Yan, Jinmian Ye, Meng Ma, Ping Wang",NIPS,2019,https://arxiv.org/abs/1909.08174
Global Sparse Momentum SGD for Pruning Very Deep Neural Networks,"Xiaohan Ding, Guiguang Ding, Xiangxin Zhou, Yuchen Guo, Jungong Han, Ji Liu",NIPS,2019,https://arxiv.org/abs/1909.12778
HYDRA: Pruning adversarially robust neural networks,"Vikash Sehwag, Shiqi Wang, Prateek Mittal, Suman Jana",NIPS,2020,https://arxiv.org/abs/2002.10509
Logarithmic pruning is all you need,"Laurent Orseau, Marcus Hutter, Omar Rivasplata",NIPS,2020,https://arxiv.org/abs/2006.12156
Model Compression with Adversarial Robustness: A Unified Optimization Framework,"Shupeng Gui, Haotao Wang, Chen Yu, Haichuan Yang, Zhangyang Wang, Ji Liu",NIPS,2019,https://arxiv.org/abs/1902.03538
Movement Pruning: Adaptive Sparsity by Fine-Tuning,"Victor Sanh, Thomas Wolf, Alexander M. Rush",NIPS,2020,https://arxiv.org/abs/2005.07683
Network Pruning via Transformable Architecture Search,"Xuanyi Dong, Yi Yang",NIPS,2019,https://arxiv.org/abs/1905.09717
Neuron Merging: Compensating for Pruned Neurons,"Woojeong Kim, Suhyun Kim, Mincheol Park, Geonseok Jeon",NIPS,2020,https://arxiv.org/abs/2010.13160
Neuron-level Structured Pruning using Polarization Regularizer,"Tao Zhuang, Zhixuan Zhang, Yuheng Huang, Xiaoyi Zeng, Kai Shuang, Xiang Li",NIPS,2020,https://papers.nips.cc/paper/2020/file/703957b6dd9e3a7980e040bee50ded65-Paper.pdf
One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers,"Ari S. Morcos, Haonan Yu, Michela Paganini, Yuandong Tian",NIPS,2019,https://arxiv.org/abs/1906.02773
Position-based Scaled Gradient for Model Quantization and Pruning,"Jangho Kim, KiYoon Yoo, Nojun Kwak",NIPS,2020,https://arxiv.org/abs/2005.11035
Pruning Filter in Filter,"Fanxu Meng, Hao Cheng, Ke Li, Huixiang Luo, Xiaowei Guo, Guangming Lu, Xing Sun",NIPS,2020,https://arxiv.org/abs/2009.14410
Pruning neural networks without any data by iteratively conserving synaptic flow,"Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, Surya Ganguli",NIPS,2020,https://arxiv.org/abs/2006.05467
Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot,"Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, Jason D. Lee",NIPS,2020,https://arxiv.org/abs/2009.11094
SCOP: Scientific Control for Reliable Neural Network Pruning,"Yehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chunjing Xu, Chao Xu, Chang Xu",NIPS,2020,https://arxiv.org/abs/2010.10732
Storage Efficient and Dynamic Flexible Runtime Channel Pruning via Deep Reinforcement Learning,"Jianda Chen, Shangyu Chen, Sinno Jialin Pan",NIPS,2020,https://proceedings.neurips.cc/paper/2020/hash/a914ecef9c12ffdb9bede64bb703d877-Abstract.html
The Generalization-Stability Tradeoff In Neural Network Pruning,"Brian R. Bartoldson, Ari S. Morcos, Adrian Barbu, Gordon Erlebacher",NIPS,2020,https://arxiv.org/abs/1906.03728
Efficient Conformal Prediction via Cascaded Inference with Expanded Admission,"Adam Fisch, Tal Schuster, Tommi S. Jaakkola, Regina Barzilay",ICLR,2021,https://openreview.net/forum?id=tnSo6VRLmT
Dual-mode ASR: Unify and Improve Streaming ASR with Full-context Modeling,"Jiahui Yu, Wei Han, Anmol Gulati, Chung-Cheng Chiu, Bo Li, Tara N Sainath, Yonghui Wu, Ruoming Pang",ICLR,2021,https://openreview.net/forum?id=Pz_dcqfcKW8
Knowledge distillation via softmax regression representation learning,"Jing Yang, Brais Martinez, Adrian Bulat, Georgios Tzimiropoulos",ICLR,2021,https://openreview.net/forum?id=ZzwDy_wiWv
